{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding In Natural language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenized senences as training data\n",
    "tokenized_sentences = [['Hello','This','is','python','training','by','Institute'],\n",
    "             ['Hello','This','is','Java','training','by','Institute'],\n",
    "             ['Hello','This','is','Data Science','training','for','graduates'],\n",
    "             ['Hello','This','is','programming','training','']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "mymodel = Word2Vec(tokenized_sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=13, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# summarizing the loaded model\n",
    "print(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training',\n",
       " 'is',\n",
       " 'This',\n",
       " 'Hello',\n",
       " 'Institute',\n",
       " 'by',\n",
       " '',\n",
       " 'programming',\n",
       " 'graduates',\n",
       " 'for',\n",
       " 'Data Science',\n",
       " 'Java',\n",
       " 'python']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary keys\n",
    "words = list(mymodel.wv.key_to_index)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training', 'is', 'This', 'Hello', 'Institute', 'by', '', 'programming', 'graduates', 'for', 'Data Science', 'Java', 'python']\n"
     ]
    }
   ],
   "source": [
    "# summarize vocabulary\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3622725e-04  2.3643016e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588715e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633730e-03  7.3805046e-03 -1.5334726e-03\n",
      " -4.5366143e-03  6.5540504e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488189e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508893e-03 -3.4053659e-03 -9.4640255e-04  5.7685734e-03\n",
      " -7.5216386e-03 -3.9361049e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337698e-03 -1.9377422e-03\n",
      "  8.0774352e-03 -5.9308959e-03  4.5161247e-05 -4.7537349e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595871e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618264e-04 -7.6612402e-03  9.6147414e-03\n",
      "  4.9820566e-03  9.2331432e-03 -8.1579182e-03  4.4957972e-03\n",
      " -4.1370774e-03  8.2453492e-04  8.4986184e-03 -4.4621779e-03\n",
      "  4.5175003e-03 -6.7869616e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776539e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080094e-03  2.4697948e-03 -8.8802812e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459523e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705511e-03  5.7178497e-04\n",
      "  7.4419067e-03 -8.1328390e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265619e-03  5.4014279e-03  7.0526553e-03\n",
      " -5.7031228e-03  1.8588186e-03  6.0888622e-03 -4.7980524e-03\n",
      " -3.1072616e-03  6.7976285e-03  1.6314745e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777629e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173913e-03 -7.0415614e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "# access word vector for one word \"training\"\n",
    "vector = mymodel.wv.get_vector('training')\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 0.17272791266441345\n",
      "by 0.16694681346416473\n",
      "programming 0.11117953062057495\n",
      "Data Science 0.10942255705595016\n",
      "training 0.07963485270738602\n",
      "Hello 0.04131004959344864\n",
      "graduates 0.03771485015749931\n",
      "is 0.008315937593579292\n",
      "Institute -0.005896786693483591\n",
      "python -0.030302345752716064\n"
     ]
    }
   ],
   "source": [
    "#try finding most similar words for word \"Data\"\n",
    "similar_words = mymodel.wv.most_similar(\"Java\")\n",
    "for word, score in similar_words:\n",
    "    print(word, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try finding most similar words for word \"Data\"\n",
    "similar_words = mymodel.wv.doesnt_match([\"Java\",\"Institute\",\"Hello\",\"This\"])\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "sentences = [\n",
    "    ['I', 'love', 'natural', 'language', 'processing'],\n",
    "    ['Word', 'embeddings', 'are', 'useful', 'in', 'NLP'],\n",
    "    ['I', 'enjoy', 'working', 'with', 'word', 'vectors'],\n",
    "    ['Machine', 'learning', 'is', 'an', 'important', 'aspect', 'of', 'data', 'science'],\n",
    "    ['Word', 'embeddings', 'can', 'capture', 'semantic', 'relationships'],\n",
    "    ['Deep', 'learning', 'models', 'often', 'use', 'word', 'embeddings'],\n",
    "    ['Word2Vec', 'is', 'a', 'popular', 'algorithm', 'for', 'word', 'embedding'],\n",
    "    ['I', 'am', 'learning', 'Word2Vec', 'for', 'NLP'],\n",
    "    ['Word', 'embeddings', 'are', 'dense', 'vector', 'representations', 'of', 'words'],\n",
    "    ['NLP', 'tasks', 'often', 'benefit', 'from', 'pre-trained', 'word', 'embeddings']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'language': [ 0.00973708 -0.00978046 -0.00649382  0.00278359  0.00642799 -0.00537154\n",
      "  0.00275221  0.00912417 -0.00681627 -0.00610591 -0.00498472 -0.00368031\n",
      "  0.00185309  0.00968501  0.0064413   0.00040063  0.00247638  0.00843981\n",
      "  0.00912612  0.00562099  0.00594713 -0.00761882 -0.00382212 -0.00568689\n",
      "  0.00618007 -0.00225061 -0.00877997  0.0076169   0.00840086 -0.00331963\n",
      "  0.00911852 -0.00074093 -0.00362374 -0.00038817  0.00019251 -0.00350342\n",
      "  0.00281534  0.00573002  0.00686411 -0.00890567 -0.00218942 -0.00548028\n",
      "  0.00752014  0.00650113 -0.00435958  0.00232067 -0.00596084  0.00023551\n",
      "  0.0094599  -0.00260661 -0.00518894 -0.00739851 -0.00291376 -0.00086356\n",
      "  0.00352524  0.00974464 -0.00338761  0.00190258  0.00968369  0.00153214\n",
      "  0.00098226  0.00980093  0.00929687  0.00771189 -0.00617373  0.00999075\n",
      "  0.00584348  0.00907265 -0.00199469  0.00335072  0.00683613 -0.00388727\n",
      "  0.00664142  0.00256674  0.00931891 -0.00303695 -0.00310408  0.00621246\n",
      " -0.00908007 -0.00725452 -0.00650292 -0.00074808 -0.00236054  0.0068179\n",
      "  0.00923522 -0.00090759  0.00141378  0.00202134 -0.00202136 -0.00802996\n",
      "  0.00744411 -0.00429942  0.00457496  0.00909103  0.00304318  0.00314315\n",
      "  0.00406473 -0.00270497  0.00382492  0.00033875]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Accessing word vectors\n",
    "print(\"Vector for 'language':\", model.wv['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'language': [('word', 0.15937383472919464), ('representations', 0.15623889863491058), ('I', 0.15278607606887817), ('relationships', 0.14992523193359375), ('dense', 0.14473387598991394), ('useful', 0.14260095357894897), ('are', 0.13277468085289001), ('capture', 0.12229277193546295), ('with', 0.11944551765918732), ('benefit', 0.09594906121492386)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Finding most similar words\n",
    "similar_words = model.wv.most_similar('language')\n",
    "print(\"Most similar words to 'language':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'language' and 'processing': 0.15937382\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Similarity between words\n",
    "similarity_score = model.wv.similarity('language', 'word')\n",
    "print(\"Similarity between 'language' and 'processing':\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the words is not in the vocabulary.\n",
      "Vocabulary: ['embeddings', 'word', 'I', 'learning', 'Word', 'NLP', 'for', 'is', 'often', 'Word2Vec', 'of', 'are', 'in', 'useful', 'aspect', 'working', 'processing', 'with', 'language', 'vectors', 'Machine', 'natural', 'love', 'an', 'important', 'enjoy', 'pre-trained', 'from', 'algorithm', 'benefit', 'tasks', 'words', 'representations', 'vector', 'dense', 'am', 'embedding', 'popular', 'science', 'a', 'use', 'models', 'Deep', 'relationships', 'semantic', 'capture', 'can', 'data']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Similarity between words that are not in the vocabulary\n",
    "try:\n",
    "    similarity_score = model.wv.similarity('language', 'English')\n",
    "    print(\"Similarity between these words:\", similarity_score)\n",
    "except KeyError as e:\n",
    "    print(f\"At least one of the words is not in the vocabulary.\")\n",
    "\n",
    "# Accessing vocabulary\n",
    "vocabulary = list(model.wv.index_to_key)\n",
    "print(\"Vocabulary:\", vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Word2Vec Showcase!\n",
      "1. Find similar words\n",
      "2. Calculate similarity between words\n",
      "Enter your choice (1/2): 1\n",
      "Enter a word to find similar words: Word\n",
      "Words similar to 'Word':\n",
      "important: 0.17818447947502136\n",
      "embedding: 0.16392144560813904\n",
      "capture: 0.14955472946166992\n",
      "NLP: 0.13167978823184967\n",
      "Deep: 0.07775183022022247\n",
      "is: 0.07505468279123306\n",
      "word: 0.06803048402070999\n",
      "dense: 0.0677378922700882\n",
      "an: 0.04822660610079765\n",
      "benefit: 0.047279421240091324\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample data\n",
    "sentences = [\n",
    "    ['I', 'love', 'mango', 'and', 'apple', 'fruits'],\n",
    "    ['Word', 'embeddings', 'are', 'useful', 'in', 'NLP'],\n",
    "    ['I', 'enjoy', 'working', 'with', 'word', 'vectors'],\n",
    "    ['Machine', 'learning', 'is', 'an', 'important', 'aspect', 'of', 'data', 'science'],\n",
    "    ['Word', 'embeddings', 'can', 'capture', 'semantic', 'relationships'],\n",
    "    ['Deep', 'learning', 'models', 'often', 'use', 'word', 'embeddings'],\n",
    "    ['Word2Vec', 'is', 'a', 'popular', 'algorithm', 'for', 'word', 'embedding'],\n",
    "    ['I', 'am', 'learning', 'Word2Vec', 'for', 'NLP'],\n",
    "    ['Word', 'embeddings', 'are', 'dense', 'vector', 'representations', 'of', 'words'],\n",
    "    ['NLP', 'tasks', 'often', 'benefit', 'from', 'pre-trained', 'word', 'embeddings']\n",
    "]\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Application: Finding similar words\n",
    "def find_similar_words(word):\n",
    "    try:\n",
    "        similar_words = model.wv.most_similar(word)\n",
    "        print(f\"Words similar to '{word}':\")\n",
    "        for similar_word, similarity_score in similar_words:\n",
    "            print(f\"{similar_word}: {similarity_score}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{word}' not found in vocabulary.\")\n",
    "\n",
    "# Application: Similarity between words\n",
    "def calculate_similarity(word1, word2):\n",
    "    try:\n",
    "        similarity_score = model.wv.similarity(word1, word2)\n",
    "        print(f\"Similarity between '{word1}' and '{word2}': {similarity_score}\")\n",
    "    except KeyError:\n",
    "        print(\"At least one of the words is not found in vocabulary.\")\n",
    "\n",
    "# Showcase the application\n",
    "print(\"Welcome to Word2Vec Showcase!\")\n",
    "print(\"1. Find similar words\")\n",
    "print(\"2. Calculate similarity between words\")\n",
    "choice = input(\"Enter your choice (1/2): \")\n",
    "\n",
    "if choice == '1':\n",
    "    word = input(\"Enter a word to find similar words: \")\n",
    "    find_similar_words(word)\n",
    "elif choice == '2':\n",
    "    word1 = input(\"Enter the first word: \")\n",
    "    word2 = input(\"Enter the second word: \")\n",
    "    calculate_similarity(word1, word2)\n",
    "else:\n",
    "    print(\"Invalid choice. Please enter '1' or '2'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
